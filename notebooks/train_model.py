# -*- coding: utf-8 -*-
"""train_model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1K41rrykBa4_Eq-JEQFdhvRVAiFBDTCai

# ClaimCracker v2 - Model Training

This notebook trains our fake news detection model using Google Colab's GPU.
"""
# Training script for fake news detection model
# Uses DistilBERT with GPU acceleration on Colab

# Setup
from google.colab import drive
drive.mount('/content/drive')

!pip install torch transformers pandas scikit-learn tqdm

import torch
import pandas as pd
import numpy as np
from pathlib import Path
from transformers import AutoModel, AutoTokenizer
from torch import nn
from torch.utils.data import Dataset, DataLoader
from torch.optim import AdamW
from torch.optim.lr_scheduler import CosineAnnealingLR
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, f1_score
from tqdm.auto import tqdm
import matplotlib.pyplot as plt

class NewsClassifier(nn.Module):
    def __init__(
        self,
        model_name="distilbert-base-uncased",
        num_classes=2,
        dropout=0.1,
        max_length=512
    ):
        super().__init__()

        self.model_name = model_name
        self.num_classes = num_classes
        self.max_length = max_length

        self.transformer = AutoModel.from_pretrained(model_name)
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)

        self.dropout = nn.Dropout(dropout)
        self.classifier = nn.Linear(
            self.transformer.config.hidden_size,
            num_classes
        )

        self.criterion = nn.CrossEntropyLoss()

    def forward(self, input_ids, attention_mask, labels=None):
        outputs = self.transformer(
            input_ids=input_ids,
            attention_mask=attention_mask
        )

        pooled_output = outputs.last_hidden_state[:, 0]
        pooled_output = self.dropout(pooled_output)
        logits = self.classifier(pooled_output)

        result = {"logits": logits}

        if labels is not None:
            result["loss"] = self.criterion(logits, labels)

        return result

    def prepare_input(self, texts):
        return self.tokenizer(
            texts,
            max_length=self.max_length,
            padding='max_length',
            truncation=True,
            return_tensors="pt"
        )

    def save_pretrained(self, save_dir):
        save_path = Path(save_dir)
        save_path.mkdir(parents=True, exist_ok=True)

        torch.save(self.state_dict(), save_path / "model.pt")
        self.tokenizer.save_pretrained(save_path)

        config = {
            "model_name": self.model_name,
            "num_classes": self.num_classes,
            "max_length": self.max_length
        }
        torch.save(config, save_path / "config.pt")

class NewsDataset(Dataset):
    def __init__(self, texts, labels=None, model=None):
        self.texts = texts
        self.labels = labels
        self.model = model

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        text = self.texts[idx]

        inputs = self.model.tokenizer(
            text,
            max_length=self.model.max_length,
            padding='max_length',
            truncation=True,
            return_tensors="pt"
        )

        item = {
            "input_ids": inputs["input_ids"].squeeze(0),
            "attention_mask": inputs["attention_mask"].squeeze(0)
        }

        if self.labels is not None:
            item["labels"] = torch.tensor(self.labels[idx])

        return item

# Training settings
class TrainingConfig:
    def __init__(self):
        self.model_name = "distilbert-base-uncased"
        self.num_classes = 2
        self.max_length = 512
        self.dropout = 0.1
        self.batch_size = 16
        self.learning_rate = 2e-5
        self.num_epochs = 3
        self.device = "cuda" if torch.cuda.is_available() else "cpu"

# Load and prep data
df = pd.read_csv('/content/drive/MyDrive/ClaimCracker/data/Dataset.csv')

df['News_Text'] = df['News_Text'].fillna('')
df['News_Text'] = df['News_Text'].astype(str)

label_map = {'Real': 0, 'Fake': 1}
df['label_idx'] = df['Label'].map(label_map)

df = df[df['News_Text'].str.strip() != '']

train_df, val_df = train_test_split(
    df,
    test_size=0.1,
    random_state=42,
    stratify=df['Label']
)

print(f"Training samples: {len(train_df)}")
print(f"Validation samples: {len(val_df)}")
print("\nClass distribution:")
print(train_df['Label'].value_counts(normalize=True))

print("\nSample text type:", type(train_df['News_Text'].iloc[0]))
print("Text length range:",
      f"Min: {train_df['News_Text'].str.len().min()}, ",
      f"Max: {train_df['News_Text'].str.len().max()}")

# Initialize model and data loaders
config = TrainingConfig()

model = NewsClassifier(
    model_name=config.model_name,
    num_classes=config.num_classes,
    dropout=config.dropout,
    max_length=config.max_length
).to(config.device)

train_texts = train_df['News_Text'].tolist()
train_labels = train_df['label_idx'].tolist()
val_texts = val_df['News_Text'].tolist()
val_labels = val_df['label_idx'].tolist()

train_dataset = NewsDataset(train_texts, train_labels, model)
val_dataset = NewsDataset(val_texts, val_labels, model)

train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=config.batch_size)

# Training setup
optimizer = AdamW(model.parameters(), lr=config.learning_rate)
scheduler = CosineAnnealingLR(optimizer, T_max=config.num_epochs)
history = {
    "train_loss": [],
    "val_loss": [],
    "val_accuracy": [],
    "val_f1": []
}

# Train model
for epoch in range(config.num_epochs):
    print(f"\nEpoch {epoch + 1}/{config.num_epochs}")

    model.train()
    total_loss = 0

    for batch in tqdm(train_loader, desc="Training"):
        batch = {k: v.to(config.device) for k, v in batch.items()}

        optimizer.zero_grad()
        outputs = model(**batch)
        loss = outputs["loss"]

        loss.backward()
        optimizer.step()

        total_loss += loss.item()

    train_loss = total_loss / len(train_loader)

    # Validate
    model.eval()
    total_loss = 0
    all_preds = []
    all_labels = []

    with torch.no_grad():
        for batch in tqdm(val_loader, desc="Evaluating"):
            batch = {k: v.to(config.device) for k, v in batch.items()}
            labels = batch.pop("labels")

            outputs = model(**batch, labels=labels)
            loss = outputs["loss"]
            logits = outputs["logits"]

            preds = torch.argmax(logits, dim=1)

            total_loss += loss.item()
            all_preds.extend(preds.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())

    val_loss = total_loss / len(val_loader)
    val_accuracy = accuracy_score(all_labels, all_preds)
    val_f1 = f1_score(all_labels, all_preds, average='weighted')

    history["train_loss"].append(train_loss)
    history["val_loss"].append(val_loss)
    history["val_accuracy"].append(val_accuracy)
    history["val_f1"].append(val_f1)

    scheduler.step()

    print(f"Train Loss: {train_loss:.4f}")
    print(f"Val Loss: {val_loss:.4f}")
    print(f"Val Accuracy: {val_accuracy:.4f}")
    print(f"Val F1: {val_f1:.4f}")

# Plot training results
plt.figure(figsize=(12, 4))

plt.subplot(1, 2, 1)
plt.plot(history['train_loss'], label='Train')
plt.plot(history['val_loss'], label='Val')
plt.title('Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(history['val_accuracy'], label='Accuracy')
plt.plot(history['val_f1'], label='F1')
plt.title('Validation Metrics')
plt.xlabel('Epoch')
plt.ylabel('Score')
plt.legend()

plt.tight_layout()
plt.show()

# Save trained model
save_path = Path('/content/drive/MyDrive/ClaimCracker/models/best_model')
model.save_pretrained(save_path)
print(f"Model saved to {save_path}")

def predict_text(text, model, device):
    model.eval()
    inputs = model.prepare_input([text])
    inputs = {k: v.to(device) for k, v in inputs.items()}

    with torch.no_grad():
        outputs = model(**inputs)
        logits = outputs["logits"]
        probs = torch.softmax(logits, dim=1)
        pred = torch.argmax(logits, dim=1)

    return {
        "prediction": "Real" if pred.item() == 0 else "Fake",
        "confidence": probs[0][pred.item()].item()
    }

# Quick test
text = "The UNGA President emphasizes Pakistan's responsibility to advocate more assertively for the Kashmir issue at the United Nations."
result = predict_text(text, model, config.device)
print(f"Prediction: {result['prediction']}")
print(f"Confidence: {result['confidence']:.2%}")

# Test various news types
test_texts = [
    """The Federal Reserve announced today it will maintain interest rates at their current level, citing stable inflation and employment numbers. Chair Jerome Powell stated in a press conference that the decision was unanimous among board members.""",
    """BREAKING: Scientists discover that drinking coffee while standing causes instant memory loss! Big Coffee companies have been hiding this truth for decades. Share before they delete this!""",
    """Local community center hosts annual fundraiser this weekend. Event will feature live music and food vendors from the area.""",
    """A new study published in Nature reveals correlation between atmospheric carbon levels and marine biodiversity changes over the past decade, suggesting potential impacts on ocean ecosystems."""
]

for text in test_texts:
    result = predict_text(text, model, config.device)
    print("\nText:", text[:100], "...")
    print(f"Prediction: {result['prediction']}")
    print(f"Confidence: {result['confidence']:.2%}")

# Performance benchmarking
def benchmark_inference(model, text, num_runs=100):
    _ = predict_text(text, model, config.device)

    start = time.time()
    for _ in range(num_runs):
        _ = predict_text(text, model, config.device)
    end = time.time()

    avg_time = (end - start) / num_runs
    print(f"Average inference time: {avg_time*1000:.2f}ms")

    if torch.cuda.is_available():
        memory_allocated = torch.cuda.memory_allocated() / 1024**2
        print(f"GPU Memory allocated: {memory_allocated:.2f}MB")

    model_size = sum(p.numel() for p in model.parameters()) * 4 / 1024**2
    print(f"Model size: {model_size:.2f}MB")

test_text = "Sample news article for benchmarking."
print("Original Model Performance:")
benchmark_inference(model, test_text)

# Model optimization
def optimize_for_inference(model):
    model.eval()

    try:
        sample_text = "Sample text for optimization"
        inputs = model.prepare_input([sample_text])
        inputs = {k: v.to(model.device) for k, v in inputs.items()}

        traced_model = torch.jit.trace(model, (inputs['input_ids'], inputs['attention_mask']))
        traced_model = torch.jit.optimize_for_inference(traced_model)
        print("Model successfully optimized with TorchScript")
        return traced_model
    except Exception as e:
        print(f"TorchScript optimization failed: {e}")
        print("Falling back to original model")
        return model

print("Optimizing model for inference...")
optimized_model = optimize_for_inference(model)

print("\nOptimized Model Performance:")
benchmark_inference(optimized_model, test_text)

!pip install onnx onnxruntime

# Export model to ONNX format
def export_to_onnx(model, save_path):
    dummy_input = model.prepare_input(["Sample text"])
    dummy_input = {k: v.to(config.device) for k, v in dummy_input.items()}

    torch.onnx.export(
        model,
        (dummy_input["input_ids"], dummy_input["attention_mask"]),
        save_path,
        input_names=["input_ids", "attention_mask"],
        output_names=["logits"],
        dynamic_axes={
            "input_ids": {0: "batch_size", 1: "sequence"},
            "attention_mask": {0: "batch_size", 1: "sequence"},
            "logits": {0: "batch_size"}
        },
        opset_version=14,
        do_constant_folding=True,
        export_params=True,
        verbose=False
    )
    print(f"Model exported to {save_path}")

onnx_path = save_path / "model.onnx"
export_to_onnx(model, onnx_path)

import onnx
onnx_model = onnx.load(onnx_path)
onnx.checker.check_model(onnx_model)
print("ONNX model verified successfully!")

onnx_size = onnx_path.stat().st_size / (1024 * 1024)
print(f"ONNX model size: {onnx_size:.2f}MB")

# Batch inference for production
@torch.no_grad()
def batch_inference(model, texts, batch_size=32):
    model.eval()
    results = []

    for i in range(0, len(texts), batch_size):
        batch_texts = texts[i:i + batch_size]
        inputs = model.prepare_input(batch_texts)
        inputs = {k: v.to(config.device) for k, v in inputs.items()}

        outputs = model(**inputs)
        logits = outputs["logits"]
        probs = torch.softmax(logits, dim=1)
        preds = torch.argmax(logits, dim=1)

        preds = preds.cpu()
        probs = probs.cpu()

        for idx, (pred, prob) in enumerate(zip(preds, probs)):
            pred_idx = pred.item()
            results.append({
                "prediction": "Real" if pred_idx == 0 else "Fake",
                "confidence": prob[pred_idx].item()
            })

    return results

# Test batch processing
test_texts = ["Text " + str(i) for i in range(100)]
print("\nBatch Inference Test:")
start = time.time()
results = batch_inference(model, test_texts)
end = time.time()
print(f"Processed {len(test_texts)} texts in {(end-start)*1000:.2f}ms")
print(f"Average time per text: {(end-start)*1000/len(test_texts):.2f}ms")

print("\nSample results:")
for i, result in enumerate(results[:5]):
    print(f"Text {i}: {result['prediction']} (Confidence: {result['confidence']:.2%})")

# Export production-ready model
def export_final_model():
    export_dir = Path('/content/drive/MyDrive/ClaimCracker/final_model')
    export_dir.mkdir(exist_ok=True, parents=True)

    torch.save(model.state_dict(), export_dir / 'model.pt')

    config = {
        'hidden_size': 768,
        'num_classes': 2,
        'dropout': 0.1
    }
    torch.save(config, export_dir / 'config.pt')

    model_code = """
import torch
from torch import nn
from transformers import DistilBertModel, AutoTokenizer

class NewsClassifier(nn.Module):
    def __init__(self, hidden_size=768, num_classes=2, dropout=0.1):
        super().__init__()
        self.bert = DistilBertModel.from_pretrained('distilbert-base-uncased')
        self.dropout = nn.Dropout(dropout)
        self.classifier = nn.Linear(hidden_size, num_classes)

    def forward(self, input_ids, attention_mask):
        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)
        pooled_output = outputs[0][:, 0]
        pooled_output = self.dropout(pooled_output)
        logits = self.classifier(pooled_output)
        return {"logits": logits}

    def prepare_input(self, texts):
        tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')
        inputs = tokenizer(texts, padding=True, truncation=True, return_tensors="pt")
        return inputs
"""

    with open(export_dir / 'model_architecture.py', 'w') as f:
        f.write(model_code)

    test_script = """
import torch
from pathlib import Path
from transformers import AutoTokenizer
from model_architecture import NewsClassifier

def load_model(model_dir):
    config = torch.load(Path(model_dir) / "config.pt")
    model = NewsClassifier(**config)
    model.load_state_dict(torch.load(Path(model_dir) / "model.pt"))
    model.eval()
    return model

def test_prediction(text, model_dir="final_model"):
    model = load_model(model_dir)
    inputs = model.prepare_input([text])

    with torch.no_grad():
        outputs = model(**inputs)
        logits = outputs["logits"]
        probs = torch.softmax(logits, dim=1)
        pred = torch.argmax(logits, dim=1)

    prediction = "Real" if pred.item() == 0 else "Fake"
    confidence = probs[0][pred[0]].item()

    return {
        "prediction": prediction,
        "confidence": confidence
    }

if __name__ == "__main__":
    texts = [
        "Reuters reports global markets show steady growth.",
        "SHOCKING: Scientists hide the truth about water!"
    ]

    for text in texts:
        result = test_prediction(text)
        print(f"\\nText: {text}")
        print(f"Prediction: {result['prediction']}")
        print(f"Confidence: {result['confidence']:.2%}")
"""

    with open(export_dir / 'test_model.py', 'w') as f:
        f.write(test_script)

    readme = """
# ClaimCracker v2 - Fake News Detection Model

## Model Details
- Architecture: DistilBERT-based classifier
- Performance: 96.03% validation accuracy
- Inference speed: ~13.94ms per text
- Size: Check model.pt and config.pt

## Usage
1. Load the model:
```python
from test_model import load_model, test_prediction

# Simple prediction
result = test_prediction("Your news text here")
print(f"Prediction: {result['prediction']}")
print(f"Confidence: {result['confidence']:.2%}")
```

## Files
- model.pt: Model weights
- config.pt: Model configuration
- model_architecture.py: Model class definition
- test_model.py: Example usage and testing
"""

    with open(export_dir / 'README.md', 'w') as f:
        f.write(readme)

    print(f"Exported model files to '{export_dir}':")
    print("- model.pt")
    print("- config.pt")
    print("- model_architecture.py")
    print("- test_model.py")
    print("- README.md")

export_final_model()

# Final model testing
realistic_texts = [
    "Constable killed another injured after car rams into Lahore check post,car belonged to the son of Mian Munir who happens to be a relative samdhi of Maryam Nawaz Sharif",
    "SHOCKING: Scientists find that drinking water is actually bad for you! Share this before they delete it!",
    "Local community center announces new after-school programs starting next month.",
    "New research published in Science shows promising results in renewable energy efficiency.",
    "WARNING!! They don't want you to know this secret to instant weight loss!!!"
] * 20

print("Batch Processing Test with Realistic Examples:")
start = time.time()
results = batch_inference(model, realistic_texts)
end = time.time()

total_time = (end-start) * 1000
print(f"\nProcessed {len(realistic_texts)} texts in {total_time:.2f}ms")
print(f"Average time per text: {total_time/len(realistic_texts):.2f}ms")

print("\nSample results:")
for i in range(5):
    print(f"\nText: {realistic_texts[i][:100]}...")
    print(f"Prediction: {results[i]['prediction']}")
    print(f"Confidence: {results[i]['confidence']:.2%}")